{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf664784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Sample DataFrame setup\n",
    "data = {\n",
    "    'Ticket': range(1, 21),\n",
    "    'Business Service': [\n",
    "        'Microsoft Outlook', 'Microsoft Windows', 'Laptop', 'Python',\n",
    "        'voice log', 'crm service', 'cwhh vdi', 'vantage agent portal',\n",
    "        'Microsoft Excel', 'MS Excel', 'Excel 365', 'outlook email',\n",
    "        'Outlook client', 'Windows 10', 'Microsoft Windows 11',\n",
    "        'laptop hardware', 'laptop support', 'python development',\n",
    "        'Python scripting', 'CRM system'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Method 1: LLM-based approach\n",
    "def llm_based_standardization(df, chat_function):\n",
    "    \"\"\"\n",
    "    Use LLM to group similar services and standardize them\n",
    "    \"\"\"\n",
    "    # Get unique services and their counts\n",
    "    service_counts = df['Business Service'].value_counts()\n",
    "    unique_services = service_counts.index.tolist()\n",
    "    \n",
    "    # Create prompt for LLM to group similar services\n",
    "    prompt = f\"\"\"\n",
    "    I have a list of business services that need to be standardized. Please group similar services together and for each group, suggest the most representative name (preferably the one that appears most frequently).\n",
    "\n",
    "    Services with their counts:\n",
    "    {dict(service_counts)}\n",
    "\n",
    "    Please return a JSON object where:\n",
    "    - Keys are the standardized service names\n",
    "    - Values are lists of all variations that should map to that standardized name\n",
    "\n",
    "    Example format:\n",
    "    {{\n",
    "        \"Microsoft Excel\": [\"Microsoft Excel\", \"MS Excel\", \"Excel 365\"],\n",
    "        \"Microsoft Outlook\": [\"Microsoft Outlook\", \"outlook email\", \"Outlook client\"]\n",
    "    }}\n",
    "\n",
    "    Focus on grouping services that refer to the same underlying technology or service.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Call the LLM function\n",
    "        response = chat_function(prompt)\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        # Note: You might need to extract JSON from the response text\n",
    "        standardization_map = json.loads(response)\n",
    "        \n",
    "        # Create a mapping from original to standardized\n",
    "        service_mapping = {}\n",
    "        for standard_name, variations in standardization_map.items():\n",
    "            for variation in variations:\n",
    "                service_mapping[variation] = standard_name\n",
    "        \n",
    "        # Apply mapping to create new column\n",
    "        df['New Services'] = df['Business Service'].map(\n",
    "            lambda x: service_mapping.get(x, x)\n",
    "        )\n",
    "        \n",
    "        return df, service_mapping\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM processing: {e}\")\n",
    "        return df, {}\n",
    "\n",
    "# Method 2: Traditional NLP approach\n",
    "def traditional_nlp_standardization(df, similarity_threshold=80):\n",
    "    \"\"\"\n",
    "    Use fuzzy matching and pattern recognition to standardize services\n",
    "    \"\"\"\n",
    "    # Get service counts\n",
    "    service_counts = df['Business Service'].value_counts()\n",
    "    unique_services = list(service_counts.index)\n",
    "    \n",
    "    # Preprocessing function\n",
    "    def preprocess_service(service):\n",
    "        # Convert to lowercase, remove extra spaces, common prefixes/suffixes\n",
    "        service = service.lower().strip()\n",
    "        service = re.sub(r'\\bmicrosoft\\b|\\bms\\b', 'microsoft', service)\n",
    "        service = re.sub(r'\\b365\\b|\\boffice\\b', '', service).strip()\n",
    "        service = re.sub(r'\\s+', ' ', service)\n",
    "        return service\n",
    "    \n",
    "    # Create groups of similar services\n",
    "    groups = []\n",
    "    used_services = set()\n",
    "    \n",
    "    for service in unique_services:\n",
    "        if service in used_services:\n",
    "            continue\n",
    "            \n",
    "        # Find similar services\n",
    "        similar_services = [service]\n",
    "        used_services.add(service)\n",
    "        \n",
    "        for other_service in unique_services:\n",
    "            if other_service in used_services:\n",
    "                continue\n",
    "                \n",
    "            # Check similarity using multiple methods\n",
    "            similarity_scores = [\n",
    "                fuzz.ratio(preprocess_service(service), preprocess_service(other_service)),\n",
    "                fuzz.partial_ratio(service.lower(), other_service.lower()),\n",
    "                fuzz.token_sort_ratio(service.lower(), other_service.lower()),\n",
    "                fuzz.token_set_ratio(service.lower(), other_service.lower())\n",
    "            ]\n",
    "            \n",
    "            max_similarity = max(similarity_scores)\n",
    "            \n",
    "            # Also check for keyword overlap\n",
    "            service_words = set(preprocess_service(service).split())\n",
    "            other_words = set(preprocess_service(other_service).split())\n",
    "            word_overlap = len(service_words & other_words) / max(len(service_words), len(other_words))\n",
    "            \n",
    "            if max_similarity >= similarity_threshold or word_overlap >= 0.6:\n",
    "                similar_services.append(other_service)\n",
    "                used_services.add(other_service)\n",
    "        \n",
    "        if len(similar_services) > 1:\n",
    "            groups.append(similar_services)\n",
    "    \n",
    "    # For each group, find the service with highest count\n",
    "    service_mapping = {}\n",
    "    for group in groups:\n",
    "        # Get counts for services in this group\n",
    "        group_counts = {service: service_counts[service] for service in group}\n",
    "        # Find the service with maximum count\n",
    "        standard_service = max(group_counts, key=group_counts.get)\n",
    "        \n",
    "        # Map all services in group to the standard one\n",
    "        for service in group:\n",
    "            service_mapping[service] = standard_service\n",
    "    \n",
    "    # Apply mapping\n",
    "    df['New Services'] = df['Business Service'].map(\n",
    "        lambda x: service_mapping.get(x, x)\n",
    "    )\n",
    "    \n",
    "    return df, service_mapping, groups\n",
    "\n",
    "# Method 3: Hybrid approach\n",
    "def hybrid_standardization(df, chat_function, similarity_threshold=75):\n",
    "    \"\"\"\n",
    "    Combine traditional NLP for initial grouping and LLM for validation/refinement\n",
    "    \"\"\"\n",
    "    # Step 1: Use traditional NLP for initial grouping\n",
    "    df_temp, initial_mapping, groups = traditional_nlp_standardization(\n",
    "        df.copy(), similarity_threshold\n",
    "    )\n",
    "    \n",
    "    # Step 2: Use LLM to validate and refine the groups\n",
    "    if groups:\n",
    "        prompt = f\"\"\"\n",
    "        I've used fuzzy matching to group similar business services. Please review these groups and suggest improvements:\n",
    "\n",
    "        Groups found:\n",
    "        {json.dumps(groups, indent=2)}\n",
    "\n",
    "        Service counts:\n",
    "        {dict(df['Business Service'].value_counts())}\n",
    "\n",
    "        Please return a JSON object with refined groupings where:\n",
    "        - Keys are the best representative names for each service category\n",
    "        - Values are lists of all variations that should map to that name\n",
    "        \n",
    "        Validate that the groupings make sense semantically.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = chat_function(prompt)\n",
    "            refined_mapping = json.loads(response)\n",
    "            \n",
    "            # Create final mapping\n",
    "            final_service_mapping = {}\n",
    "            for standard_name, variations in refined_mapping.items():\n",
    "                for variation in variations:\n",
    "                    final_service_mapping[variation] = standard_name\n",
    "            \n",
    "            df['New Services'] = df['Business Service'].map(\n",
    "                lambda x: final_service_mapping.get(x, x)\n",
    "            )\n",
    "            \n",
    "            return df, final_service_mapping\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in LLM refinement: {e}\")\n",
    "            return df_temp, initial_mapping\n",
    "    \n",
    "    return df_temp, initial_mapping\n",
    "\n",
    "# Example usage and comparison\n",
    "def compare_methods(df):\n",
    "    \"\"\"\n",
    "    Compare the effectiveness of different methods\n",
    "    \"\"\"\n",
    "    print(\"Original services:\")\n",
    "    print(df['Business Service'].value_counts())\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Traditional NLP method\n",
    "    df_traditional, mapping_traditional, groups = traditional_nlp_standardization(df.copy())\n",
    "    print(\"Traditional NLP Results:\")\n",
    "    print(\"Groups found:\", groups)\n",
    "    print(\"Standardized services:\")\n",
    "    print(df_traditional['New Services'].value_counts())\n",
    "    print(f\"Reduced from {df['Business Service'].nunique()} to {df_traditional['New Services'].nunique()} unique services\")\n",
    "    \n",
    "    return df_traditional, mapping_traditional\n",
    "\n",
    "# Mock chat function for testing (replace with your actual LLM endpoint)\n",
    "def mock_chat_function(prompt):\n",
    "    \"\"\"\n",
    "    Mock LLM function - replace this with your actual chat function\n",
    "    \"\"\"\n",
    "    # This is a mock response - your actual LLM should analyze the prompt\n",
    "    mock_response = '''\n",
    "    {\n",
    "        \"Microsoft Excel\": [\"Microsoft Excel\", \"MS Excel\", \"Excel 365\"],\n",
    "        \"Microsoft Outlook\": [\"Microsoft Outlook\", \"outlook email\", \"Outlook client\"],\n",
    "        \"Microsoft Windows\": [\"Microsoft Windows\", \"Windows 10\", \"Microsoft Windows 11\"],\n",
    "        \"Laptop\": [\"Laptop\", \"laptop hardware\", \"laptop support\"],\n",
    "        \"Python\": [\"Python\", \"python development\", \"Python scripting\"],\n",
    "        \"CRM Service\": [\"crm service\", \"CRM system\"],\n",
    "        \"Voice Log\": [\"voice log\"],\n",
    "        \"CWHH VDI\": [\"cwhh vdi\"],\n",
    "        \"Vantage Agent Portal\": [\"vantage agent portal\"]\n",
    "    }\n",
    "    '''\n",
    "    return mock_response\n",
    "\n",
    "# Run comparison\n",
    "if __name__ == \"__main__\":\n",
    "    # Test traditional method\n",
    "    df_result, mapping = compare_methods(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(\"Final DataFrame with standardized services:\")\n",
    "    print(df_result[['Business Service', 'New Services']].head(10))\n",
    "    \n",
    "    # Test LLM method (uncomment when you have actual chat function)\n",
    "    # df_llm, mapping_llm = llm_based_standardization(df.copy(), your_chat_function)\n",
    "    # print(\"LLM Results:\", df_llm['New Services'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "def llm_service_standardization(df: pd.DataFrame, chat_function, \n",
    "                              service_column: str = 'Business Service',\n",
    "                              new_column: str = 'New Services') -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Standardize business services using LLM with robust error handling\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the service data\n",
    "        chat_function: Your LLM chat function\n",
    "        service_column: Name of column containing services to standardize\n",
    "        new_column: Name of new column for standardized services\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (updated_dataframe, service_mapping_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get service counts\n",
    "    service_counts = df[service_column].value_counts()\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_standardization_prompt(service_counts)\n",
    "    \n",
    "    try:\n",
    "        # Call your LLM\n",
    "        print(\"Calling LLM for service standardization...\")\n",
    "        response = chat_function(prompt)\n",
    "        print(f\"LLM Response received: {len(response)} characters\")\n",
    "        \n",
    "        # Parse the response\n",
    "        standardization_map = parse_llm_response(response)\n",
    "        \n",
    "        # Validate the mapping\n",
    "        standardization_map = validate_mapping(standardization_map, service_counts.index.tolist())\n",
    "        \n",
    "        # Apply the mapping\n",
    "        df = apply_service_mapping(df, standardization_map, service_column, new_column)\n",
    "        \n",
    "        # Print results\n",
    "        print_results(df, service_column, new_column, standardization_map)\n",
    "        \n",
    "        return df, standardization_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM standardization: {e}\")\n",
    "        # Fallback: copy original column\n",
    "        df[new_column] = df[service_column]\n",
    "        return df, {}\n",
    "\n",
    "def create_standardization_prompt(service_counts) -> str:\n",
    "    \"\"\"Create a detailed prompt for the LLM\"\"\"\n",
    "    \n",
    "    services_text = \"\\n\".join([f\"- {service}: {count} occurrences\" \n",
    "                              for service, count in service_counts.items()])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are helping standardize business service names in a IT ticketing system. \n",
    "\n",
    "TASK: Group similar services together and choose the best representative name for each group.\n",
    "\n",
    "SERVICES AND THEIR FREQUENCIES:\n",
    "{services_text}\n",
    "\n",
    "RULES:\n",
    "1. Group services that refer to the same underlying technology/service\n",
    "2. For each group, choose the name with the HIGHEST frequency as the standard\n",
    "3. If frequencies are equal, choose the most descriptive/official name\n",
    "4. Keep unrelated services separate\n",
    "5. Be conservative - only group if you're confident they're the same service\n",
    "\n",
    "EXAMPLES of what should be grouped:\n",
    "- \"Microsoft Excel\", \"MS Excel\", \"Excel 365\" → all refer to Excel\n",
    "- \"Microsoft Outlook\", \"Outlook email\", \"Outlook client\" → all refer to Outlook\n",
    "- \"Windows 10\", \"Microsoft Windows 11\", \"Microsoft Windows\" → all refer to Windows OS\n",
    "\n",
    "EXAMPLES of what should NOT be grouped:\n",
    "- \"Python\" and \"Java\" → different programming languages\n",
    "- \"Laptop\" and \"Desktop\" → different hardware types\n",
    "- \"CRM\" and \"ERP\" → different software categories\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return ONLY a valid JSON object with this exact structure:\n",
    "{{\n",
    "    \"Standard Service Name 1\": [\"variation1\", \"variation2\", \"variation3\"],\n",
    "    \"Standard Service Name 2\": [\"variation1\", \"variation2\"],\n",
    "    \"Ungrouped Service\": [\"Ungrouped Service\"]\n",
    "}}\n",
    "\n",
    "IMPORTANT: \n",
    "- Include ALL original services in your response\n",
    "- Each service should appear exactly once\n",
    "- Use the exact service names from the input list\n",
    "- Return only the JSON, no additional text\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def parse_llm_response(response: str) -> Dict:\n",
    "    \"\"\"Parse LLM response and extract JSON mapping\"\"\"\n",
    "    \n",
    "    # Try to find JSON in the response\n",
    "    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group()\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            print(f\"Problematic JSON: {json_str[:200]}...\")\n",
    "    \n",
    "    # If direct parsing fails, try to clean the response\n",
    "    try:\n",
    "        # Remove markdown code blocks if present\n",
    "        cleaned = re.sub(r'```json\\s*|\\s*```', '', response)\n",
    "        cleaned = re.sub(r'```\\s*|\\s*```', '', cleaned)\n",
    "        \n",
    "        # Find the JSON object\n",
    "        start = cleaned.find('{')\n",
    "        end = cleaned.rfind('}') + 1\n",
    "        \n",
    "        if start != -1 and end > start:\n",
    "            json_str = cleaned[start:end]\n",
    "            return json.loads(json_str)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning response: {e}\")\n",
    "    \n",
    "    raise ValueError(\"Could not parse valid JSON from LLM response\")\n",
    "\n",
    "def validate_mapping(mapping: Dict, original_services: List[str]) -> Dict:\n",
    "    \"\"\"Validate and fix the LLM mapping\"\"\"\n",
    "    \n",
    "    # Flatten all mapped services\n",
    "    mapped_services = set()\n",
    "    for variations in mapping.values():\n",
    "        mapped_services.update(variations)\n",
    "    \n",
    "    # Find missing services\n",
    "    original_set = set(original_services)\n",
    "    missing_services = original_set - mapped_services\n",
    "    \n",
    "    # Add missing services as standalone entries\n",
    "    for service in missing_services:\n",
    "        mapping[service] = [service]\n",
    "        print(f\"Added missing service: {service}\")\n",
    "    \n",
    "    # Remove any services not in original list\n",
    "    cleaned_mapping = {}\n",
    "    for standard_name, variations in mapping.items():\n",
    "        valid_variations = [v for v in variations if v in original_set]\n",
    "        if valid_variations:\n",
    "            cleaned_mapping[standard_name] = valid_variations\n",
    "    \n",
    "    return cleaned_mapping\n",
    "\n",
    "def apply_service_mapping(df: pd.DataFrame, mapping: Dict, \n",
    "                         service_column: str, new_column: str) -> pd.DataFrame:\n",
    "    \"\"\"Apply the standardization mapping to the dataframe\"\"\"\n",
    "    \n",
    "    # Create reverse mapping: original_service -> standard_service\n",
    "    service_map = {}\n",
    "    for standard_name, variations in mapping.items():\n",
    "        for variation in variations:\n",
    "            service_map[variation] = standard_name\n",
    "    \n",
    "    # Apply mapping\n",
    "    df[new_column] = df[service_column].map(service_map)\n",
    "    \n",
    "    # Handle any unmapped services (shouldn't happen with validation)\n",
    "    unmapped_mask = df[new_column].isna()\n",
    "    if unmapped_mask.any():\n",
    "        print(f\"Warning: {unmapped_mask.sum()} services couldn't be mapped\")\n",
    "        df.loc[unmapped_mask, new_column] = df.loc[unmapped_mask, service_column]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_results(df: pd.DataFrame, original_col: str, new_col: str, mapping: Dict):\n",
    "    \"\"\"Print standardization results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STANDARDIZATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    original_unique = df[original_col].nunique()\n",
    "    new_unique = df[new_col].nunique()\n",
    "    reduction = original_unique - new_unique\n",
    "    \n",
    "    print(f\"Original unique services: {original_unique}\")\n",
    "    print(f\"Standardized unique services: {new_unique}\")\n",
    "    print(f\"Reduction: {reduction} services ({reduction/original_unique*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nGROUPINGS MADE:\")\n",
    "    for standard_name, variations in mapping.items():\n",
    "        if len(variations) > 1:\n",
    "            print(f\"\\n'{standard_name}' ← {variations}\")\n",
    "    \n",
    "    print(f\"\\nFINAL SERVICE DISTRIBUTION:\")\n",
    "    print(df[new_col].value_counts().head(10))\n",
    "\n",
    "# USAGE EXAMPLE WITH YOUR CHAT FUNCTION\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Example of how to use with your actual chat function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Ticket': range(1, 21),\n",
    "        'Business Service': [\n",
    "            'Microsoft Outlook', 'Microsoft Windows', 'Laptop', 'Python',\n",
    "            'voice log', 'crm service', 'cwhh vdi', 'vantage agent portal',\n",
    "            'Microsoft Excel', 'MS Excel', 'Excel 365', 'outlook email',\n",
    "            'Outlook client', 'Windows 10', 'Microsoft Windows 11',\n",
    "            'laptop hardware', 'laptop support', 'python development',\n",
    "            'Python scripting', 'CRM system'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Replace this with your actual chat function\n",
    "    def your_chat_function(prompt):\n",
    "        \"\"\"\n",
    "        Replace this with your actual LLM endpoint\n",
    "        \"\"\"\n",
    "        # Example: return chat(prompt)\n",
    "        # or: return openai.chat.completions.create(...)\n",
    "        # or: return your_llm_api_call(prompt)\n",
    "        \n",
    "        # For now, using a mock response\n",
    "        return '''\n",
    "        {\n",
    "            \"Microsoft Excel\": [\"Microsoft Excel\", \"MS Excel\", \"Excel 365\"],\n",
    "            \"Microsoft Outlook\": [\"Microsoft Outlook\", \"outlook email\", \"Outlook client\"],\n",
    "            \"Microsoft Windows\": [\"Microsoft Windows\", \"Windows 10\", \"Microsoft Windows 11\"],\n",
    "            \"Laptop\": [\"Laptop\", \"laptop hardware\", \"laptop support\"],\n",
    "            \"Python\": [\"Python\", \"python development\", \"Python scripting\"],\n",
    "            \"CRM service\": [\"crm service\", \"CRM system\"],\n",
    "            \"voice log\": [\"voice log\"],\n",
    "            \"cwhh vdi\": [\"cwhh vdi\"],\n",
    "            \"vantage agent portal\": [\"vantage agent portal\"]\n",
    "        }\n",
    "        '''\n",
    "    \n",
    "    # Run the standardization\n",
    "    df_result, mapping = llm_service_standardization(\n",
    "        df=df,\n",
    "        chat_function=your_chat_function,  # Replace with your actual function\n",
    "        service_column='Business Service',\n",
    "        new_column='New Services'\n",
    "    )\n",
    "    \n",
    "    return df_result, mapping\n",
    "\n",
    "# INTEGRATION PATTERNS FOR DIFFERENT LLM PROVIDERS\n",
    "\n",
    "def openai_integration_example():\n",
    "    \"\"\"Example for OpenAI API\"\"\"\n",
    "    import openai\n",
    "    \n",
    "    def chat_with_openai(prompt):\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1  # Low temperature for consistent results\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "def anthropic_integration_example():\n",
    "    \"\"\"Example for Anthropic Claude API\"\"\"\n",
    "    import anthropic\n",
    "    \n",
    "    client = anthropic.Anthropic(api_key=\"your-api-key\")\n",
    "    \n",
    "    def chat_with_claude(prompt):\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=2000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "def custom_endpoint_example():\n",
    "    \"\"\"Example for custom API endpoint\"\"\"\n",
    "    import requests\n",
    "    \n",
    "    def chat_with_custom_api(prompt):\n",
    "        response = requests.post(\n",
    "            \"https://your-api-endpoint.com/chat\",\n",
    "            json={\"prompt\": prompt, \"max_tokens\": 2000},\n",
    "            headers={\"Authorization\": \"Bearer your-token\"}\n",
    "        )\n",
    "        return response.json()[\"response\"]\n",
    "\n",
    "# Run the example\n",
    "if __name__ == \"__main__\":\n",
    "    df_result, mapping = example_usage()\n",
    "    \n",
    "    print(\"\\nSample of final results:\")\n",
    "    print(df_result[['Business Service', 'New Services']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e653ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-Based Incident Notes Extraction\n",
    "# Extract \"How broke?\" and \"How resolved?\" from Notes field using LLM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: DATA LOADING AND EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_explore_data(file_path=None, sample_data=True):\n",
    "    \"\"\"\n",
    "    Load the incident data and perform initial exploration\n",
    "    \"\"\"\n",
    "    if sample_data:\n",
    "        # Create realistic sample data for demonstration\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        sample_notes = [\n",
    "            \"User reported Outlook crashes when opening large emails. Investigated and found PST file corruption. Rebuilt PST file and updated Outlook to latest version. Issue resolved.\",\n",
    "            \"Server experiencing high CPU usage causing website timeouts. Identified memory leak in application code. Restarted services and applied hotfix patch. Performance back to normal.\",\n",
    "            \"Printer showing paper jam error but no visible jam. Found sensor dirty from toner dust. Cleaned sensors and calibrated printer. Printing working correctly now.\",\n",
    "            \"Network connection dropping randomly throughout office. Discovered faulty switch port. Replaced network switch and updated firmware. Connectivity stable.\",\n",
    "            \"Blue screen errors on Windows 10 machine with memory fault codes. Ran diagnostics and found bad RAM stick. Replaced defective memory module. System stable.\",\n",
    "            \"Excel files opening corrupted with missing data. Office installation was damaged. Uninstalled and reinstalled Office 365 suite. Files opening properly.\",\n",
    "            \"VPN authentication failing for remote users. Certificate had expired on server. Renewed SSL certificates and updated VPN configuration. Remote access restored.\",\n",
    "            \"Database queries running extremely slow during peak hours. Found missing indexes on large tables. Added proper indexes and optimized queries. Performance improved significantly.\",\n",
    "            \"Phone system not receiving incoming calls. SIP trunk configuration was incorrect. Reconfigured trunk settings with carrier. Call routing working normally.\",\n",
    "            \"Backup jobs failing with disk space errors. Log files consuming excessive storage. Cleaned old logs and increased partition size. Backups completing successfully.\",\n",
    "            \"Email server rejecting messages with spam filter errors. Whitelist settings were too restrictive. Adjusted spam filter rules and retrained filters. Email flow restored.\",\n",
    "            \"Laptop keyboard keys sticking and not responding. Coffee spill damage to internal components. Replaced keyboard assembly and cleaned logic board. Typing functional again.\",\n",
    "            \"Website loading very slowly for all users. CDN cache was not updating properly. Purged CDN cache and fixed cache headers. Site speed back to normal.\",\n",
    "            \"Scanner not detected by workstation. USB driver conflicts after Windows update. Uninstalled conflicting drivers and reinstalled scanner software. Scanning operational.\",\n",
    "            \"Database connection timeouts in web application. Connection pool was exhausted during high traffic. Increased pool size and added connection retry logic. App responding normally.\",\n",
    "            \"Monitor displaying purple tint and color distortion. Graphics driver was outdated and corrupted. Updated to latest display drivers and calibrated monitor. Colors accurate now.\",\n",
    "            \"File server inaccessible with permission denied errors. Active Directory replication had failed. Forced AD sync and reset computer account. File access restored.\",\n",
    "            \"Antivirus blocking legitimate software installation. False positive detection in security rules. Added software to exclusion list and updated virus definitions. Installation proceeded.\",\n",
    "            \"Conference room camera not working for video calls. Camera driver was missing after system update. Reinstalled camera drivers and updated firmware. Video calls functional.\",\n",
    "            \"POS system crashing during transaction processing. Database lock timeouts under load. Optimized database queries and increased timeout values. Transactions processing smoothly.\"\n",
    "        ]\n",
    "        \n",
    "        # Extend to create more samples\n",
    "        extended_notes = []\n",
    "        for i in range(200):\n",
    "            note = sample_notes[i % len(sample_notes)]\n",
    "            # Add some variation\n",
    "            variations = [\n",
    "                note,\n",
    "                note.replace(\"User reported\", \"Customer called about\"),\n",
    "                note.replace(\"Issue resolved\", \"Problem fixed\"),\n",
    "                note.replace(\"Found\", \"Discovered\"),\n",
    "                note.replace(\"working correctly\", \"functioning properly\")\n",
    "            ]\n",
    "            extended_notes.append(variations[i % len(variations)])\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Ticket': [f'INC{2000 + i}' for i in range(len(extended_notes))],\n",
    "            'Service Rep': np.random.choice(['Alice Johnson', 'Bob Smith', 'Carol Davis', 'David Wilson', 'Eva Brown'], len(extended_notes)),\n",
    "            'description': [note.split('.')[0] + '.' for note in extended_notes],  # First sentence as description\n",
    "            'close_notes': extended_notes  # Full note as close_notes\n",
    "        })\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Rename 'Notes' to 'close_notes' if needed\n",
    "        if 'Notes' in df.columns and 'close_notes' not in df.columns:\n",
    "            df = df.rename(columns={'Notes': 'close_notes'})\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(df)} incidents\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nSample close_notes:\")\n",
    "    for i, note in enumerate(df['close_notes'].head(3)):\n",
    "        print(f\"{i+1}. {note[:100]}...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_and_explore_data()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: LLM EXTRACTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def extract_how_broke_and_resolved(notes: str, chat_function) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Use LLM to extract 'how broke' and 'how resolved' from incident notes\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are analyzing IT incident notes to extract two key pieces of information.\n",
    "\n",
    "INCIDENT NOTES:\n",
    "{notes}\n",
    "\n",
    "Extract the following information in exactly this format:\n",
    "\n",
    "HOW BROKE: [What went wrong or failed - max 15 words, technical focus]\n",
    "HOW RESOLVED: [How the issue was fixed - max 15 words, solution focus]\n",
    "\n",
    "GUIDELINES:\n",
    "- Focus on technical root cause for \"HOW BROKE\"\n",
    "- Focus on the actual solution for \"HOW RESOLVED\"\n",
    "- Use simple, clear technical language\n",
    "- If information is unclear or missing, use \"Unknown\"\n",
    "- Avoid user names, timestamps, non-technical details\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Example 1:\n",
    "Notes: \"User reported printer paper jam. Found sensor dirty from toner. Cleaned sensors. Printing works now.\"\n",
    "HOW BROKE: printer sensor dirty from toner dust\n",
    "HOW RESOLVED: cleaned dirty sensors and calibrated printer\n",
    "\n",
    "Example 2:\n",
    "Notes: \"Server down with 500 errors. Memory leak in application. Restarted services and applied patch.\"\n",
    "HOW BROKE: memory leak causing server errors\n",
    "HOW RESOLVED: restarted services and applied hotfix patch\n",
    "\n",
    "Example 3:\n",
    "Notes: \"Please help urgently! Something is broken!\"\n",
    "HOW BROKE: Unknown\n",
    "HOW RESOLVED: Unknown\n",
    "\n",
    "Now extract from the provided incident notes:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = chat_function(prompt).strip()\n",
    "        \n",
    "        # Parse the response\n",
    "        how_broke = \"Unknown\"\n",
    "        how_resolved = \"Unknown\"\n",
    "        \n",
    "        # Look for the structured format\n",
    "        broke_match = re.search(r'HOW BROKE:\\s*(.+)', response, re.IGNORECASE)\n",
    "        resolved_match = re.search(r'HOW RESOLVED:\\s*(.+)', response, re.IGNORECASE)\n",
    "        \n",
    "        if broke_match:\n",
    "            how_broke = broke_match.group(1).strip()\n",
    "            # Clean up and limit words\n",
    "            how_broke = ' '.join(how_broke.split()[:15])\n",
    "        \n",
    "        if resolved_match:\n",
    "            how_resolved = resolved_match.group(1).strip()\n",
    "            # Clean up and limit words  \n",
    "            how_resolved = ' '.join(how_resolved.split()[:15])\n",
    "        \n",
    "        # Validate outputs\n",
    "        if len(how_broke) > 100 or not how_broke or how_broke.lower() in ['unknown', 'unclear', 'not specified']:\n",
    "            how_broke = \"Unknown\"\n",
    "        \n",
    "        if len(how_resolved) > 100 or not how_resolved or how_resolved.lower() in ['unknown', 'unclear', 'not specified']:\n",
    "            how_resolved = \"Unknown\"\n",
    "            \n",
    "        return how_broke, how_resolved\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting from notes: {e}\")\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "\n",
    "def batch_extract_incidents(df: pd.DataFrame, chat_function, \n",
    "                          batch_size: int = 1, sample_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process incident notes in batches to extract how broke and how resolved\n",
    "    \"\"\"\n",
    "    df_work = df.copy()\n",
    "    \n",
    "    if sample_size:\n",
    "        df_work = df_work.head(sample_size)\n",
    "        print(f\"Processing sample of {sample_size} incidents for demonstration\")\n",
    "    \n",
    "    print(f\"Extracting information from {len(df_work)} incident notes...\")\n",
    "    \n",
    "    how_broke_list = []\n",
    "    how_resolved_list = []\n",
    "    \n",
    "    for i, notes in enumerate(df_work['close_notes']):\n",
    "        try:\n",
    "            how_broke, how_resolved = extract_how_broke_and_resolved(notes, chat_function)\n",
    "            how_broke_list.append(how_broke)\n",
    "            how_resolved_list.append(how_resolved)\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(df_work)} incidents\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing incident {i + 1}: {e}\")\n",
    "            how_broke_list.append(\"Unknown\")\n",
    "            how_resolved_list.append(\"Unknown\")\n",
    "    \n",
    "    # Add new columns\n",
    "    df_work['how broke'] = how_broke_list\n",
    "    df_work['how resolved'] = how_resolved_list\n",
    "    \n",
    "    print(\"Extraction completed!\")\n",
    "    return df_work\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: MOCK CHAT FUNCTION FOR DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "def mock_chat_function(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Mock chat function for demonstration. Replace with your actual chat function.\n",
    "    \"\"\"\n",
    "    # Extract the notes section from the prompt\n",
    "    notes_match = re.search(r'INCIDENT NOTES:\\s*(.+?)\\s*Extract', prompt, re.DOTALL)\n",
    "    if not notes_match:\n",
    "        return \"HOW BROKE: Unknown\\nHOW RESOLVED: Unknown\"\n",
    "    \n",
    "    notes = notes_match.group(1).strip().lower()\n",
    "    \n",
    "    # Pattern matching for common scenarios\n",
    "    how_broke = \"Unknown\"\n",
    "    how_resolved = \"Unknown\"\n",
    "    \n",
    "    # Analyze what broke\n",
    "    if 'outlook' in notes and 'crash' in notes:\n",
    "        how_broke = \"outlook application crashes when opening emails\"\n",
    "    elif 'server' in notes and ('cpu' in notes or 'memory leak' in notes):\n",
    "        how_broke = \"server memory leak causing high cpu usage\"\n",
    "    elif 'printer' in notes and ('jam' in notes or 'sensor' in notes):\n",
    "        how_broke = \"printer sensor dirty from toner dust\"\n",
    "    elif 'network' in notes and ('dropping' in notes or 'connection' in notes):\n",
    "        how_broke = \"faulty network switch port causing disconnections\"\n",
    "    elif 'blue screen' in notes and 'memory' in notes:\n",
    "        how_broke = \"defective ram causing blue screen errors\"\n",
    "    elif 'excel' in notes and 'corrupt' in notes:\n",
    "        how_broke = \"office installation damaged corrupting excel files\"\n",
    "    elif 'vpn' in notes and ('authentication' in notes or 'certificate' in notes):\n",
    "        how_broke = \"expired ssl certificate blocking vpn authentication\"\n",
    "    elif 'database' in notes and 'slow' in notes:\n",
    "        how_broke = \"missing database indexes causing slow queries\"\n",
    "    elif 'backup' in notes and ('disk space' in notes or 'storage' in notes):\n",
    "        how_broke = \"excessive log files consuming backup storage space\"\n",
    "    elif 'email' in notes and ('spam' in notes or 'filter' in notes):\n",
    "        how_broke = \"restrictive spam filter blocking legitimate emails\"\n",
    "    elif 'keyboard' in notes and ('stick' in notes or 'spill' in notes):\n",
    "        how_broke = \"coffee spill damage to keyboard components\"\n",
    "    elif 'website' in notes and ('slow' in notes or 'cdn' in notes):\n",
    "        how_broke = \"cdn cache not updating causing slow loading\"\n",
    "    elif 'scanner' in notes and ('driver' in notes or 'usb' in notes):\n",
    "        how_broke = \"usb driver conflicts after windows update\"\n",
    "    elif 'timeout' in notes and ('connection' in notes or 'pool' in notes):\n",
    "        how_broke = \"database connection pool exhausted during peak traffic\"\n",
    "    elif 'monitor' in notes and ('color' in notes or 'display' in notes):\n",
    "        how_broke = \"outdated graphics driver causing color distortion\"\n",
    "    elif 'file server' in notes and 'permission' in notes:\n",
    "        how_broke = \"active directory replication failure blocking file access\"\n",
    "    elif 'antivirus' in notes and ('blocking' in notes or 'false positive' in notes):\n",
    "        how_broke = \"antivirus false positive blocking legitimate software installation\"\n",
    "    elif 'camera' in notes and ('driver' in notes or 'missing' in notes):\n",
    "        how_broke = \"missing camera driver after system update\"\n",
    "    elif 'pos' in notes and ('crash' in notes or 'transaction' in notes):\n",
    "        how_broke = \"database lock timeouts under transaction load\"\n",
    "    else:\n",
    "        # Extract key failure words\n",
    "        failure_keywords = ['crash', 'error', 'fail', 'down', 'slow', 'corrupt', 'timeout', 'block']\n",
    "        for keyword in failure_keywords:\n",
    "            if keyword in notes:\n",
    "                how_broke = f\"system {keyword} detected\"\n",
    "                break\n",
    "    \n",
    "    # Analyze how it was resolved\n",
    "    if 'rebuilt pst' in notes or 'updated outlook' in notes:\n",
    "        how_resolved = \"rebuilt pst file and updated outlook version\"\n",
    "    elif 'restarted services' in notes and 'patch' in notes:\n",
    "        how_resolved = \"restarted services and applied hotfix patch\"\n",
    "    elif 'cleaned sensors' in notes:\n",
    "        how_resolved = \"cleaned dirty sensors and calibrated printer\"\n",
    "    elif 'replaced' in notes and 'switch' in notes:\n",
    "        how_resolved = \"replaced faulty network switch and updated firmware\"\n",
    "    elif 'replaced' in notes and 'memory' in notes:\n",
    "        how_resolved = \"replaced defective memory module\"\n",
    "    elif 'reinstalled office' in notes:\n",
    "        how_resolved = \"uninstalled and reinstalled office 365 suite\"\n",
    "    elif 'renewed' in notes and 'certificate' in notes:\n",
    "        how_resolved = \"renewed ssl certificates and updated vpn configuration\"\n",
    "    elif 'added indexes' in notes or 'optimized queries' in notes:\n",
    "        how_resolved = \"added database indexes and optimized queries\"\n",
    "    elif 'increased partition' in notes or 'cleaned logs' in notes:\n",
    "        how_resolved = \"cleaned old logs and increased partition size\"\n",
    "    elif 'adjusted' in notes and 'filter' in notes:\n",
    "        how_resolved = \"adjusted spam filter rules and retrained filters\"\n",
    "    elif 'replaced keyboard' in notes:\n",
    "        how_resolved = \"replaced keyboard assembly and cleaned logic board\"\n",
    "    elif 'purged cache' in notes:\n",
    "        how_resolved = \"purged cdn cache and fixed cache headers\"\n",
    "    elif 'reinstalled' in notes and 'driver' in notes:\n",
    "        how_resolved = \"uninstalled conflicting drivers and reinstalled scanner software\"\n",
    "    elif 'increased pool' in notes:\n",
    "        how_resolved = \"increased connection pool size and added retry logic\"\n",
    "    elif 'updated' in notes and 'driver' in notes:\n",
    "        how_resolved = \"updated display drivers and calibrated monitor\"\n",
    "    elif 'forced sync' in notes or 'reset account' in notes:\n",
    "        how_resolved = \"forced active directory sync and reset computer account\"\n",
    "    elif 'exclusion list' in notes:\n",
    "        how_resolved = \"added software to exclusion list and updated definitions\"\n",
    "    elif 'reinstalled camera' in notes:\n",
    "        how_resolved = \"reinstalled camera drivers and updated firmware\"\n",
    "    elif 'optimized database' in notes:\n",
    "        how_resolved = \"optimized database queries and increased timeout values\"\n",
    "    else:\n",
    "        # Extract key resolution words\n",
    "        resolution_keywords = ['restart', 'update', 'replace', 'install', 'configure', 'clean', 'repair']\n",
    "        for keyword in resolution_keywords:\n",
    "            if keyword in notes:\n",
    "                how_resolved = f\"performed {keyword} operation\"\n",
    "                break\n",
    "    \n",
    "    return f\"HOW BROKE: {how_broke}\\nHOW RESOLVED: {how_resolved}\"\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: TESTING THE EXTRACTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING EXTRACTION FUNCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with sample notes\n",
    "sample_notes = [\n",
    "    \"User reported Outlook crashes when opening large emails. Found PST file corruption. Rebuilt PST file and updated Outlook. Issue resolved.\",\n",
    "    \"Server experiencing high CPU usage. Memory leak in application code. Restarted services and applied hotfix patch. Performance normal.\",\n",
    "    \"Please help urgently! Something is broken but not sure what!\",\n",
    "    \"Printer paper jam error but no visible jam. Sensor dirty from toner. Cleaned sensors and calibrated printer. Working correctly now.\"\n",
    "]\n",
    "\n",
    "print(\"Sample extractions:\")\n",
    "for i, notes in enumerate(sample_notes):\n",
    "    how_broke, how_resolved = extract_how_broke_and_resolved(notes, mock_chat_function)\n",
    "    print(f\"\\n{i+1}. Notes: {notes}\")\n",
    "    print(f\"   How broke: {how_broke}\")\n",
    "    print(f\"   How resolved: {how_resolved}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: MAIN PROCESSING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def process_incident_notes(df: pd.DataFrame, chat_function, sample_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function to process incident notes and extract how broke/resolved information\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STARTING INCIDENT NOTES PROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Validate input data\n",
    "    if 'close_notes' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'close_notes' column\")\n",
    "    \n",
    "    # Check for empty notes\n",
    "    empty_notes = df['close_notes'].isna().sum()\n",
    "    if empty_notes > 0:\n",
    "        print(f\"Warning: {empty_notes} incidents have empty notes\")\n",
    "        df['close_notes'] = df['close_notes'].fillna(\"No notes provided\")\n",
    "    \n",
    "    # Process the notes\n",
    "    df_result = batch_extract_incidents(df, chat_function, sample_size=sample_size)\n",
    "    \n",
    "    print(\"\\nProcessing completed successfully!\")\n",
    "    return df_result\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: ANALYSIS AND VISUALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_extraction_results(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze the results of the extraction process\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ANALYSIS OF EXTRACTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_incidents = len(df)\n",
    "    \n",
    "    # Basic statistics\n",
    "    how_broke_unknown = (df['how broke'] == 'Unknown').sum()\n",
    "    how_resolved_unknown = (df['how resolved'] == 'Unknown').sum()\n",
    "    \n",
    "    print(f\"Total incidents processed: {total_incidents}\")\n",
    "    print(f\"'How broke' unknown: {how_broke_unknown} ({how_broke_unknown/total_incidents*100:.1f}%)\")\n",
    "    print(f\"'How resolved' unknown: {how_resolved_unknown} ({how_resolved_unknown/total_incidents*100:.1f}%)\")\n",
    "    \n",
    "    # Top failure categories\n",
    "    print(f\"\\nTop 10 'How broke' categories:\")\n",
    "    top_broke = df['how broke'].value_counts().head(10)\n",
    "    for category, count in top_broke.items():\n",
    "        print(f\"  {category}: {count} ({count/total_incidents*100:.1f}%)\")\n",
    "    \n",
    "    # Top resolution categories\n",
    "    print(f\"\\nTop 10 'How resolved' categories:\")\n",
    "    top_resolved = df['how resolved'].value_counts().head(10)\n",
    "    for category, count in top_resolved.items():\n",
    "        print(f\"  {category}: {count} ({count/total_incidents*100:.1f}%)\")\n",
    "    \n",
    "    # Visualizations\n",
    "    create_visualizations(df)\n",
    "    \n",
    "    return top_broke, top_resolved\n",
    "\n",
    "def create_visualizations(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create visualizations for the extraction results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot 1: How broke distribution\n",
    "    plt.subplot(2, 3, 1)\n",
    "    top_broke = df['how broke'].value_counts().head(10)\n",
    "    plt.pie(top_broke.values, labels=[label[:30] + '...' if len(label) > 30 else label for label in top_broke.index], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Top 10 \"How Broke\" Categories Distribution')\n",
    "    \n",
    "    # Plot 2: How resolved distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    top_resolved = df['how resolved'].value_counts().head(10)\n",
    "    plt.pie(top_resolved.values, labels=[label[:30] + '...' if len(label) > 30 else label for label in top_resolved.index], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Top 10 \"How Resolved\" Categories Distribution')\n",
    "    \n",
    "    # Plot 3: How broke frequency\n",
    "    plt.subplot(2, 3, 3)\n",
    "    top_broke.plot(kind='barh')\n",
    "    plt.title('Frequency of Top 10 \"How Broke\" Categories')\n",
    "    plt.xlabel('Number of Incidents')\n",
    "    \n",
    "    # Plot 4: How resolved frequency\n",
    "    plt.subplot(2, 3, 4)\n",
    "    top_resolved.plot(kind='barh')\n",
    "    plt.title('Frequency of Top 10 \"How Resolved\" Categories')\n",
    "    plt.xlabel('Number of Incidents')\n",
    "    \n",
    "    # Plot 5: Unknown percentage by service rep\n",
    "    plt.subplot(2, 3, 5)\n",
    "    rep_stats = df.groupby('Service Rep').agg({\n",
    "        'how broke': lambda x: (x == 'Unknown').sum() / len(x) * 100,\n",
    "        'how resolved': lambda x: (x == 'Unknown').sum() / len(x) * 100\n",
    "    })\n",
    "    rep_stats.plot(kind='bar')\n",
    "    plt.title('Unknown % by Service Rep')\n",
    "    plt.ylabel('Percentage Unknown')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(['How Broke', 'How Resolved'])\n",
    "    \n",
    "    # Plot 6: Word count distribution\n",
    "    plt.subplot(2, 3, 6)\n",
    "    word_counts_broke = df['how broke'].apply(lambda x: len(x.split()) if x != 'Unknown' else 0)\n",
    "    word_counts_resolved = df['how resolved'].apply(lambda x: len(x.split()) if x != 'Unknown' else 0)\n",
    "    \n",
    "    plt.hist([word_counts_broke, word_counts_resolved], bins=15, alpha=0.7, label=['How Broke', 'How Resolved'])\n",
    "    plt.title('Word Count Distribution')\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: EXPORT FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def export_results(df: pd.DataFrame, output_path: str = 'incident_extraction_results'):\n",
    "    \"\"\"\n",
    "    Export results to CSV and generate summary report\n",
    "    \"\"\"\n",
    "    # Export main dataset\n",
    "    df.to_csv(f'{output_path}.csv', index=False)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_report = {\n",
    "        'total_incidents': len(df),\n",
    "        'how_broke_unknown_count': (df['how broke'] == 'Unknown').sum(),\n",
    "        'how_resolved_unknown_count': (df['how resolved'] == 'Unknown').sum(),\n",
    "        'top_how_broke': df['how broke'].value_counts().head(10).to_dict(),\n",
    "        'top_how_resolved': df['how resolved'].value_counts().head(10).to_dict(),\n",
    "        'service_rep_stats': df.groupby('Service Rep').agg({\n",
    "            'how broke': lambda x: (x == 'Unknown').sum(),\n",
    "            'how resolved': lambda x: (x == 'Unknown').sum()\n",
    "        }).to_dict()\n",
    "    }\n",
    "    \n",
    "    with open(f'{output_path}_summary.json', 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    \n",
    "    print(f\"Results exported to {output_path}.csv and {output_path}_summary.json\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def your_chat_function(prompt):\n",
    "    \"\"\"\n",
    "    Replace this with your actual LLM chat function\n",
    "    \n",
    "    Example integrations:\n",
    "    - return chat(prompt)\n",
    "    - return openai_client.chat.completions.create(...)\n",
    "    - return anthropic_client.messages.create(...)\n",
    "    \"\"\"\n",
    "    return mock_chat_function(prompt)\n",
    "\n",
    "# Run the main processing pipeline\n",
    "print(\"Starting the incident notes extraction pipeline...\")\n",
    "\n",
    "# Process the data (using sample for demonstration)\n",
    "df_final = process_incident_notes(\n",
    "    df=df, \n",
    "    chat_function=your_chat_function,  # Replace with your actual function\n",
    "    sample_size=50  # Remove this to process all data\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 9: DISPLAY RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display sample results\n",
    "sample_results = df_final[['Ticket', 'Service Rep', 'close_notes', 'how broke', 'how resolved']].head(10)\n",
    "print(\"\\nSample of extracted information:\")\n",
    "for _, row in sample_results.iterrows():\n",
    "    print(f\"\\nTicket: {row['Ticket']}\")\n",
    "    print(f\"Notes: {row['close_notes'][:100]}...\")\n",
    "    print(f\"How broke: {row['how broke']}\")\n",
    "    print(f\"How resolved: {row['how resolved']}\")\n",
    "\n",
    "# Run analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_broke, top_resolved = analyze_extraction_results(df_final)\n",
    "\n",
    "# Export results\n",
    "export_results(df_final)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo use with your actual data:\")\n",
    "print(\"1. Replace 'your_chat_function' with your actual LLM chat function\")\n",
    "print(\"2. Load your CSV file: df = pd.read_csv('your_file.csv')\")\n",
    "print(\"3. Ensure your data has 'close_notes' or 'Notes' column\")\n",
    "print(\"4. Run: df_result = process_incident_notes(df, your_chat_function)\")\n",
    "print(\"5. Analyze results and export as needed\")\n",
    "\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(f\"- 'how broke': Technical description of what failed (max 15 words)\")\n",
    "print(f\"- 'how resolved': Description of how issue was fixed (max 15 words)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
