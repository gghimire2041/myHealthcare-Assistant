{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf664784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Sample DataFrame setup\n",
    "data = {\n",
    "    'Ticket': range(1, 21),\n",
    "    'Business Service': [\n",
    "        'Microsoft Outlook', 'Microsoft Windows', 'Laptop', 'Python',\n",
    "        'voice log', 'crm service', 'cwhh vdi', 'vantage agent portal',\n",
    "        'Microsoft Excel', 'MS Excel', 'Excel 365', 'outlook email',\n",
    "        'Outlook client', 'Windows 10', 'Microsoft Windows 11',\n",
    "        'laptop hardware', 'laptop support', 'python development',\n",
    "        'Python scripting', 'CRM system'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Method 1: LLM-based approach\n",
    "def llm_based_standardization(df, chat_function):\n",
    "    \"\"\"\n",
    "    Use LLM to group similar services and standardize them\n",
    "    \"\"\"\n",
    "    # Get unique services and their counts\n",
    "    service_counts = df['Business Service'].value_counts()\n",
    "    unique_services = service_counts.index.tolist()\n",
    "    \n",
    "    # Create prompt for LLM to group similar services\n",
    "    prompt = f\"\"\"\n",
    "    I have a list of business services that need to be standardized. Please group similar services together and for each group, suggest the most representative name (preferably the one that appears most frequently).\n",
    "\n",
    "    Services with their counts:\n",
    "    {dict(service_counts)}\n",
    "\n",
    "    Please return a JSON object where:\n",
    "    - Keys are the standardized service names\n",
    "    - Values are lists of all variations that should map to that standardized name\n",
    "\n",
    "    Example format:\n",
    "    {{\n",
    "        \"Microsoft Excel\": [\"Microsoft Excel\", \"MS Excel\", \"Excel 365\"],\n",
    "        \"Microsoft Outlook\": [\"Microsoft Outlook\", \"outlook email\", \"Outlook client\"]\n",
    "    }}\n",
    "\n",
    "    Focus on grouping services that refer to the same underlying technology or service.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Call the LLM function\n",
    "        response = chat_function(prompt)\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        # Note: You might need to extract JSON from the response text\n",
    "        standardization_map = json.loads(response)\n",
    "        \n",
    "        # Create a mapping from original to standardized\n",
    "        service_mapping = {}\n",
    "        for standard_name, variations in standardization_map.items():\n",
    "            for variation in variations:\n",
    "                service_mapping[variation] = standard_name\n",
    "        \n",
    "        # Apply mapping to create new column\n",
    "        df['New Services'] = df['Business Service'].map(\n",
    "            lambda x: service_mapping.get(x, x)\n",
    "        )\n",
    "        \n",
    "        return df, service_mapping\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM processing: {e}\")\n",
    "        return df, {}\n",
    "\n",
    "# Method 2: Traditional NLP approach\n",
    "def traditional_nlp_standardization(df, similarity_threshold=80):\n",
    "    \"\"\"\n",
    "    Use fuzzy matching and pattern recognition to standardize services\n",
    "    \"\"\"\n",
    "    # Get service counts\n",
    "    service_counts = df['Business Service'].value_counts()\n",
    "    unique_services = list(service_counts.index)\n",
    "    \n",
    "    # Preprocessing function\n",
    "    def preprocess_service(service):\n",
    "        # Convert to lowercase, remove extra spaces, common prefixes/suffixes\n",
    "        service = service.lower().strip()\n",
    "        service = re.sub(r'\\bmicrosoft\\b|\\bms\\b', 'microsoft', service)\n",
    "        service = re.sub(r'\\b365\\b|\\boffice\\b', '', service).strip()\n",
    "        service = re.sub(r'\\s+', ' ', service)\n",
    "        return service\n",
    "    \n",
    "    # Create groups of similar services\n",
    "    groups = []\n",
    "    used_services = set()\n",
    "    \n",
    "    for service in unique_services:\n",
    "        if service in used_services:\n",
    "            continue\n",
    "            \n",
    "        # Find similar services\n",
    "        similar_services = [service]\n",
    "        used_services.add(service)\n",
    "        \n",
    "        for other_service in unique_services:\n",
    "            if other_service in used_services:\n",
    "                continue\n",
    "                \n",
    "            # Check similarity using multiple methods\n",
    "            similarity_scores = [\n",
    "                fuzz.ratio(preprocess_service(service), preprocess_service(other_service)),\n",
    "                fuzz.partial_ratio(service.lower(), other_service.lower()),\n",
    "                fuzz.token_sort_ratio(service.lower(), other_service.lower()),\n",
    "                fuzz.token_set_ratio(service.lower(), other_service.lower())\n",
    "            ]\n",
    "            \n",
    "            max_similarity = max(similarity_scores)\n",
    "            \n",
    "            # Also check for keyword overlap\n",
    "            service_words = set(preprocess_service(service).split())\n",
    "            other_words = set(preprocess_service(other_service).split())\n",
    "            word_overlap = len(service_words & other_words) / max(len(service_words), len(other_words))\n",
    "            \n",
    "            if max_similarity >= similarity_threshold or word_overlap >= 0.6:\n",
    "                similar_services.append(other_service)\n",
    "                used_services.add(other_service)\n",
    "        \n",
    "        if len(similar_services) > 1:\n",
    "            groups.append(similar_services)\n",
    "    \n",
    "    # For each group, find the service with highest count\n",
    "    service_mapping = {}\n",
    "    for group in groups:\n",
    "        # Get counts for services in this group\n",
    "        group_counts = {service: service_counts[service] for service in group}\n",
    "        # Find the service with maximum count\n",
    "        standard_service = max(group_counts, key=group_counts.get)\n",
    "        \n",
    "        # Map all services in group to the standard one\n",
    "        for service in group:\n",
    "            service_mapping[service] = standard_service\n",
    "    \n",
    "    # Apply mapping\n",
    "    df['New Services'] = df['Business Service'].map(\n",
    "        lambda x: service_mapping.get(x, x)\n",
    "    )\n",
    "    \n",
    "    return df, service_mapping, groups\n",
    "\n",
    "# Method 3: Hybrid approach\n",
    "def hybrid_standardization(df, chat_function, similarity_threshold=75):\n",
    "    \"\"\"\n",
    "    Combine traditional NLP for initial grouping and LLM for validation/refinement\n",
    "    \"\"\"\n",
    "    # Step 1: Use traditional NLP for initial grouping\n",
    "    df_temp, initial_mapping, groups = traditional_nlp_standardization(\n",
    "        df.copy(), similarity_threshold\n",
    "    )\n",
    "    \n",
    "    # Step 2: Use LLM to validate and refine the groups\n",
    "    if groups:\n",
    "        prompt = f\"\"\"\n",
    "        I've used fuzzy matching to group similar business services. Please review these groups and suggest improvements:\n",
    "\n",
    "        Groups found:\n",
    "        {json.dumps(groups, indent=2)}\n",
    "\n",
    "        Service counts:\n",
    "        {dict(df['Business Service'].value_counts())}\n",
    "\n",
    "        Please return a JSON object with refined groupings where:\n",
    "        - Keys are the best representative names for each service category\n",
    "        - Values are lists of all variations that should map to that name\n",
    "        \n",
    "        Validate that the groupings make sense semantically.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = chat_function(prompt)\n",
    "            refined_mapping = json.loads(response)\n",
    "            \n",
    "            # Create final mapping\n",
    "            final_service_mapping = {}\n",
    "            for standard_name, variations in refined_mapping.items():\n",
    "                for variation in variations:\n",
    "                    final_service_mapping[variation] = standard_name\n",
    "            \n",
    "            df['New Services'] = df['Business Service'].map(\n",
    "                lambda x: final_service_mapping.get(x, x)\n",
    "            )\n",
    "            \n",
    "            return df, final_service_mapping\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in LLM refinement: {e}\")\n",
    "            return df_temp, initial_mapping\n",
    "    \n",
    "    return df_temp, initial_mapping\n",
    "\n",
    "# Example usage and comparison\n",
    "def compare_methods(df):\n",
    "    \"\"\"\n",
    "    Compare the effectiveness of different methods\n",
    "    \"\"\"\n",
    "    print(\"Original services:\")\n",
    "    print(df['Business Service'].value_counts())\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Traditional NLP method\n",
    "    df_traditional, mapping_traditional, groups = traditional_nlp_standardization(df.copy())\n",
    "    print(\"Traditional NLP Results:\")\n",
    "    print(\"Groups found:\", groups)\n",
    "    print(\"Standardized services:\")\n",
    "    print(df_traditional['New Services'].value_counts())\n",
    "    print(f\"Reduced from {df['Business Service'].nunique()} to {df_traditional['New Services'].nunique()} unique services\")\n",
    "    \n",
    "    return df_traditional, mapping_traditional\n",
    "\n",
    "# Mock chat function for testing (replace with your actual LLM endpoint)\n",
    "def mock_chat_function(prompt):\n",
    "    \"\"\"\n",
    "    Mock LLM function - replace this with your actual chat function\n",
    "    \"\"\"\n",
    "    # This is a mock response - your actual LLM should analyze the prompt\n",
    "    mock_response = '''\n",
    "    {\n",
    "        \"Microsoft Excel\": [\"Microsoft Excel\", \"MS Excel\", \"Excel 365\"],\n",
    "        \"Microsoft Outlook\": [\"Microsoft Outlook\", \"outlook email\", \"Outlook client\"],\n",
    "        \"Microsoft Windows\": [\"Microsoft Windows\", \"Windows 10\", \"Microsoft Windows 11\"],\n",
    "        \"Laptop\": [\"Laptop\", \"laptop hardware\", \"laptop support\"],\n",
    "        \"Python\": [\"Python\", \"python development\", \"Python scripting\"],\n",
    "        \"CRM Service\": [\"crm service\", \"CRM system\"],\n",
    "        \"Voice Log\": [\"voice log\"],\n",
    "        \"CWHH VDI\": [\"cwhh vdi\"],\n",
    "        \"Vantage Agent Portal\": [\"vantage agent portal\"]\n",
    "    }\n",
    "    '''\n",
    "    return mock_response\n",
    "\n",
    "# Run comparison\n",
    "if __name__ == \"__main__\":\n",
    "    # Test traditional method\n",
    "    df_result, mapping = compare_methods(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(\"Final DataFrame with standardized services:\")\n",
    "    print(df_result[['Business Service', 'New Services']].head(10))\n",
    "    \n",
    "    # Test LLM method (uncomment when you have actual chat function)\n",
    "    # df_llm, mapping_llm = llm_based_standardization(df.copy(), your_chat_function)\n",
    "    # print(\"LLM Results:\", df_llm['New Services'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "def llm_service_standardization(df: pd.DataFrame, chat_function, \n",
    "                              service_column: str = 'Business Service',\n",
    "                              new_column: str = 'New Services') -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Standardize business services using LLM with robust error handling\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the service data\n",
    "        chat_function: Your LLM chat function\n",
    "        service_column: Name of column containing services to standardize\n",
    "        new_column: Name of new column for standardized services\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (updated_dataframe, service_mapping_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get service counts\n",
    "    service_counts = df[service_column].value_counts()\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_standardization_prompt(service_counts)\n",
    "    \n",
    "    try:\n",
    "        # Call your LLM\n",
    "        print(\"Calling LLM for service standardization...\")\n",
    "        response = chat_function(prompt)\n",
    "        print(f\"LLM Response received: {len(response)} characters\")\n",
    "        \n",
    "        # Parse the response\n",
    "        standardization_map = parse_llm_response(response)\n",
    "        \n",
    "        # Validate the mapping\n",
    "        standardization_map = validate_mapping(standardization_map, service_counts.index.tolist())\n",
    "        \n",
    "        # Apply the mapping\n",
    "        df = apply_service_mapping(df, standardization_map, service_column, new_column)\n",
    "        \n",
    "        # Print results\n",
    "        print_results(df, service_column, new_column, standardization_map)\n",
    "        \n",
    "        return df, standardization_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM standardization: {e}\")\n",
    "        # Fallback: copy original column\n",
    "        df[new_column] = df[service_column]\n",
    "        return df, {}\n",
    "\n",
    "def create_standardization_prompt(service_counts) -> str:\n",
    "    \"\"\"Create a detailed prompt for the LLM\"\"\"\n",
    "    \n",
    "    services_text = \"\\n\".join([f\"- {service}: {count} occurrences\" \n",
    "                              for service, count in service_counts.items()])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are helping standardize business service names in a IT ticketing system. \n",
    "\n",
    "TASK: Group similar services together and choose the best representative name for each group.\n",
    "\n",
    "SERVICES AND THEIR FREQUENCIES:\n",
    "{services_text}\n",
    "\n",
    "RULES:\n",
    "1. Group services that refer to the same underlying technology/service\n",
    "2. For each group, choose the name with the HIGHEST frequency as the standard\n",
    "3. If frequencies are equal, choose the most descriptive/official name\n",
    "4. Keep unrelated services separate\n",
    "5. Be conservative - only group if you're confident they're the same service\n",
    "\n",
    "EXAMPLES of what should be grouped:\n",
    "- \"Microsoft Excel\", \"MS Excel\", \"Excel 365\" → all refer to Excel\n",
    "- \"Microsoft Outlook\", \"Outlook email\", \"Outlook client\" → all refer to Outlook\n",
    "- \"Windows 10\", \"Microsoft Windows 11\", \"Microsoft Windows\" → all refer to Windows OS\n",
    "\n",
    "EXAMPLES of what should NOT be grouped:\n",
    "- \"Python\" and \"Java\" → different programming languages\n",
    "- \"Laptop\" and \"Desktop\" → different hardware types\n",
    "- \"CRM\" and \"ERP\" → different software categories\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return ONLY a valid JSON object with this exact structure:\n",
    "{{\n",
    "    \"Standard Service Name 1\": [\"variation1\", \"variation2\", \"variation3\"],\n",
    "    \"Standard Service Name 2\": [\"variation1\", \"variation2\"],\n",
    "    \"Ungrouped Service\": [\"Ungrouped Service\"]\n",
    "}}\n",
    "\n",
    "IMPORTANT: \n",
    "- Include ALL original services in your response\n",
    "- Each service should appear exactly once\n",
    "- Use the exact service names from the input list\n",
    "- Return only the JSON, no additional text\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def parse_llm_response(response: str) -> Dict:\n",
    "    \"\"\"Parse LLM response and extract JSON mapping\"\"\"\n",
    "    \n",
    "    # Try to find JSON in the response\n",
    "    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group()\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            print(f\"Problematic JSON: {json_str[:200]}...\")\n",
    "    \n",
    "    # If direct parsing fails, try to clean the response\n",
    "    try:\n",
    "        # Remove markdown code blocks if present\n",
    "        cleaned = re.sub(r'```json\\s*|\\s*```', '', response)\n",
    "        cleaned = re.sub(r'```\\s*|\\s*```', '', cleaned)\n",
    "        \n",
    "        # Find the JSON object\n",
    "        start = cleaned.find('{')\n",
    "        end = cleaned.rfind('}') + 1\n",
    "        \n",
    "        if start != -1 and end > start:\n",
    "            json_str = cleaned[start:end]\n",
    "            return json.loads(json_str)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning response: {e}\")\n",
    "    \n",
    "    raise ValueError(\"Could not parse valid JSON from LLM response\")\n",
    "\n",
    "def validate_mapping(mapping: Dict, original_services: List[str]) -> Dict:\n",
    "    \"\"\"Validate and fix the LLM mapping\"\"\"\n",
    "    \n",
    "    # Flatten all mapped services\n",
    "    mapped_services = set()\n",
    "    for variations in mapping.values():\n",
    "        mapped_services.update(variations)\n",
    "    \n",
    "    # Find missing services\n",
    "    original_set = set(original_services)\n",
    "    missing_services = original_set - mapped_services\n",
    "    \n",
    "    # Add missing services as standalone entries\n",
    "    for service in missing_services:\n",
    "        mapping[service] = [service]\n",
    "        print(f\"Added missing service: {service}\")\n",
    "    \n",
    "    # Remove any services not in original list\n",
    "    cleaned_mapping = {}\n",
    "    for standard_name, variations in mapping.items():\n",
    "        valid_variations = [v for v in variations if v in original_set]\n",
    "        if valid_variations:\n",
    "            cleaned_mapping[standard_name] = valid_variations\n",
    "    \n",
    "    return cleaned_mapping\n",
    "\n",
    "def apply_service_mapping(df: pd.DataFrame, mapping: Dict, \n",
    "                         service_column: str, new_column: str) -> pd.DataFrame:\n",
    "    \"\"\"Apply the standardization mapping to the dataframe\"\"\"\n",
    "    \n",
    "    # Create reverse mapping: original_service -> standard_service\n",
    "    service_map = {}\n",
    "    for standard_name, variations in mapping.items():\n",
    "        for variation in variations:\n",
    "            service_map[variation] = standard_name\n",
    "    \n",
    "    # Apply mapping\n",
    "    df[new_column] = df[service_column].map(service_map)\n",
    "    \n",
    "    # Handle any unmapped services (shouldn't happen with validation)\n",
    "    unmapped_mask = df[new_column].isna()\n",
    "    if unmapped_mask.any():\n",
    "        print(f\"Warning: {unmapped_mask.sum()} services couldn't be mapped\")\n",
    "        df.loc[unmapped_mask, new_column] = df.loc[unmapped_mask, service_column]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_results(df: pd.DataFrame, original_col: str, new_col: str, mapping: Dict):\n",
    "    \"\"\"Print standardization results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STANDARDIZATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    original_unique = df[original_col].nunique()\n",
    "    new_unique = df[new_col].nunique()\n",
    "    reduction = original_unique - new_unique\n",
    "    \n",
    "    print(f\"Original unique services: {original_unique}\")\n",
    "    print(f\"Standardized unique services: {new_unique}\")\n",
    "    print(f\"Reduction: {reduction} services ({reduction/original_unique*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nGROUPINGS MADE:\")\n",
    "    for standard_name, variations in mapping.items():\n",
    "        if len(variations) > 1:\n",
    "            print(f\"\\n'{standard_name}' ← {variations}\")\n",
    "    \n",
    "    print(f\"\\nFINAL SERVICE DISTRIBUTION:\")\n",
    "    print(df[new_col].value_counts().head(10))\n",
    "\n",
    "# USAGE EXAMPLE WITH YOUR CHAT FUNCTION\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Example of how to use with your actual chat function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Ticket': range(1, 21),\n",
    "        'Business Service': [\n",
    "            'Microsoft Outlook', 'Microsoft Windows', 'Laptop', 'Python',\n",
    "            'voice log', 'crm service', 'cwhh vdi', 'vantage agent portal',\n",
    "            'Microsoft Excel', 'MS Excel', 'Excel 365', 'outlook email',\n",
    "            'Outlook client', 'Windows 10', 'Microsoft Windows 11',\n",
    "            'laptop hardware', 'laptop support', 'python development',\n",
    "            'Python scripting', 'CRM system'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Replace this with your actual chat function\n",
    "    def your_chat_function(prompt):\n",
    "        \"\"\"\n",
    "        Replace this with your actual LLM endpoint\n",
    "        \"\"\"\n",
    "        # Example: return chat(prompt)\n",
    "        # or: return openai.chat.completions.create(...)\n",
    "        # or: return your_llm_api_call(prompt)\n",
    "        \n",
    "        # For now, using a mock response\n",
    "        return '''\n",
    "        {\n",
    "            \"Microsoft Excel\": [\"Microsoft Excel\", \"MS Excel\", \"Excel 365\"],\n",
    "            \"Microsoft Outlook\": [\"Microsoft Outlook\", \"outlook email\", \"Outlook client\"],\n",
    "            \"Microsoft Windows\": [\"Microsoft Windows\", \"Windows 10\", \"Microsoft Windows 11\"],\n",
    "            \"Laptop\": [\"Laptop\", \"laptop hardware\", \"laptop support\"],\n",
    "            \"Python\": [\"Python\", \"python development\", \"Python scripting\"],\n",
    "            \"CRM service\": [\"crm service\", \"CRM system\"],\n",
    "            \"voice log\": [\"voice log\"],\n",
    "            \"cwhh vdi\": [\"cwhh vdi\"],\n",
    "            \"vantage agent portal\": [\"vantage agent portal\"]\n",
    "        }\n",
    "        '''\n",
    "    \n",
    "    # Run the standardization\n",
    "    df_result, mapping = llm_service_standardization(\n",
    "        df=df,\n",
    "        chat_function=your_chat_function,  # Replace with your actual function\n",
    "        service_column='Business Service',\n",
    "        new_column='New Services'\n",
    "    )\n",
    "    \n",
    "    return df_result, mapping\n",
    "\n",
    "# INTEGRATION PATTERNS FOR DIFFERENT LLM PROVIDERS\n",
    "\n",
    "def openai_integration_example():\n",
    "    \"\"\"Example for OpenAI API\"\"\"\n",
    "    import openai\n",
    "    \n",
    "    def chat_with_openai(prompt):\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1  # Low temperature for consistent results\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "def anthropic_integration_example():\n",
    "    \"\"\"Example for Anthropic Claude API\"\"\"\n",
    "    import anthropic\n",
    "    \n",
    "    client = anthropic.Anthropic(api_key=\"your-api-key\")\n",
    "    \n",
    "    def chat_with_claude(prompt):\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=2000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "def custom_endpoint_example():\n",
    "    \"\"\"Example for custom API endpoint\"\"\"\n",
    "    import requests\n",
    "    \n",
    "    def chat_with_custom_api(prompt):\n",
    "        response = requests.post(\n",
    "            \"https://your-api-endpoint.com/chat\",\n",
    "            json={\"prompt\": prompt, \"max_tokens\": 2000},\n",
    "            headers={\"Authorization\": \"Bearer your-token\"}\n",
    "        )\n",
    "        return response.json()[\"response\"]\n",
    "\n",
    "# Run the example\n",
    "if __name__ == \"__main__\":\n",
    "    df_result, mapping = example_usage()\n",
    "    \n",
    "    print(\"\\nSample of final results:\")\n",
    "    print(df_result[['Business Service', 'New Services']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e653ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second\n",
    "# IT Incident Description Clustering and Categorization\n",
    "# This notebook processes IT incident descriptions to create standardized \"How broke?\" categories\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For clustering (install if needed: pip install scikit-learn sentence-transformers)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: DATA LOADING AND EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_explore_data(file_path=None, sample_data=True):\n",
    "    \"\"\"\n",
    "    Load the incident data and perform initial exploration\n",
    "    \"\"\"\n",
    "    if sample_data:\n",
    "        # Create sample data for demonstration\n",
    "        np.random.seed(42)\n",
    "        sample_descriptions = [\n",
    "            \"User cannot access email, Outlook keeps crashing when opening\",\n",
    "            \"Blue screen of death on Windows 10 machine, memory error\",\n",
    "            \"Network connection dropped, cannot connect to WiFi\",\n",
    "            \"Printer not working, paper jam error message displayed\",\n",
    "            \"Application crashed unexpectedly, database connection timeout\",\n",
    "            \"Hard drive making clicking noises, files corrupted\",\n",
    "            \"Monitor displaying strange colors, graphics driver issue\",\n",
    "            \"Keyboard keys not responding, spilled coffee on device\",\n",
    "            \"Server down, 500 internal error on website\",\n",
    "            \"VPN connection failed, authentication error\",\n",
    "            \"Excel file won't open, file format corrupted\",\n",
    "            \"Phone not charging, battery seems dead\",\n",
    "            \"Software installation failed, permission denied error\",\n",
    "            \"Internet browser crashing, memory leak suspected\",\n",
    "            \"Microphone not working during video call\",\n",
    "            \"Password reset not working, system locked account\",\n",
    "            \"Database query running slow, table lock timeout\",\n",
    "            \"Mobile app keeps freezing, needs force restart\",\n",
    "            \"Backup failed, disk space insufficient\",\n",
    "            \"Login page not loading, SSL certificate expired\"\n",
    "        ] * 50  # Repeat to get 1000 samples\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Ticket': [f'INC{1000 + i}' for i in range(len(sample_descriptions))],\n",
    "            'Service Rep': np.random.choice(['John Smith', 'Jane Doe', 'Mike Johnson', 'Sarah Wilson'], len(sample_descriptions)),\n",
    "            'description': sample_descriptions,\n",
    "            'close_notes': ['Resolved by ' + action for action in np.random.choice([\n",
    "                'restarting service', 'updating drivers', 'replacing hardware', \n",
    "                'clearing cache', 'reinstalling software'\n",
    "            ], len(sample_descriptions))]\n",
    "        })\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(df)} incidents\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\nSample descriptions:\")\n",
    "    for i, desc in enumerate(df['description'].head(3)):\n",
    "        print(f\"{i+1}. {desc}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_and_explore_data()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: TEXT CLEANING FUNCTIONS USING LLM\n",
    "# =============================================================================\n",
    "\n",
    "def clean_description_with_llm(description: str, chat_function) -> str:\n",
    "    \"\"\"\n",
    "    Use LLM to extract only technical failure information from description\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are helping to clean IT incident descriptions. Extract ONLY the technical information about what failed or broke.\n",
    "\n",
    "ORIGINAL DESCRIPTION:\n",
    "{description}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Keep only technical terms describing the failure/problem\n",
    "2. Remove user names, timestamps, non-technical details, politeness words\n",
    "3. Keep hardware/software names, error messages, technical symptoms\n",
    "4. If the description has no meaningful technical content, return \"meaningless\"\n",
    "5. Use maximum 15 words\n",
    "6. Focus on WHAT broke, not WHO reported it or WHEN\n",
    "\n",
    "EXAMPLES:\n",
    "Input: \"Hi John, my computer won't start this morning, can you help?\"\n",
    "Output: \"computer won't start\"\n",
    "\n",
    "Input: \"Dear IT team, I hope you're well. The printer in room 205 has a paper jam again.\"\n",
    "Output: \"printer paper jam\"\n",
    "\n",
    "Input: \"Hello, this is weird but my screen is blue and shows memory error\"\n",
    "Output: \"blue screen memory error\"\n",
    "\n",
    "Input: \"Please help me ASAP!!! This is urgent!!!\"\n",
    "Output: \"meaningless\"\n",
    "\n",
    "Return only the cleaned technical description, no explanations:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = chat_function(prompt).strip()\n",
    "        # Handle edge cases\n",
    "        if len(response) > 80 or 'meaningless' in response.lower():\n",
    "            return 'Unknown'\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning description: {e}\")\n",
    "        return 'Unknown'\n",
    "\n",
    "def batch_clean_descriptions(df: pd.DataFrame, chat_function, \n",
    "                           batch_size: int = 10, sample_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean descriptions in batches to optimize LLM calls\n",
    "    \"\"\"\n",
    "    df_work = df.copy()\n",
    "    \n",
    "    if sample_size:\n",
    "        df_work = df_work.head(sample_size)\n",
    "    \n",
    "    print(f\"Cleaning {len(df_work)} descriptions...\")\n",
    "    \n",
    "    # For demonstration, we'll clean the first few and then use pattern matching for the rest\n",
    "    # In production, you might want to clean all descriptions\n",
    "    \n",
    "    cleaned_descriptions = []\n",
    "    for i, desc in enumerate(df_work['description']):\n",
    "        if i < 20:  # Clean first 20 with LLM for demo\n",
    "            cleaned = clean_description_with_llm(desc, chat_function)\n",
    "            cleaned_descriptions.append(cleaned)\n",
    "            if i % 5 == 0:\n",
    "                print(f\"Cleaned {i+1}/{len(df_work)}\")\n",
    "        else:\n",
    "            # For demo purposes, use pattern-based cleaning for the rest\n",
    "            cleaned = pattern_based_cleaning(desc)\n",
    "            cleaned_descriptions.append(cleaned)\n",
    "    \n",
    "    df_work['cleaned_description'] = cleaned_descriptions\n",
    "    print(\"Description cleaning completed!\")\n",
    "    \n",
    "    return df_work\n",
    "\n",
    "def pattern_based_cleaning(description: str) -> str:\n",
    "    \"\"\"\n",
    "    Fallback pattern-based cleaning for efficiency\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = description.lower()\n",
    "    \n",
    "    # Remove common non-technical phrases\n",
    "    remove_patterns = [\n",
    "        r'\\b(hi|hello|dear|please|help|urgent|asap|thanks?|regards?)\\b',\n",
    "        r'\\b(morning|afternoon|evening|today|yesterday|tomorrow)\\b',\n",
    "        r'\\b(room \\d+|floor \\d+|building \\w+)\\b',\n",
    "        r'\\b(user|employee|staff|person) \\w+\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in remove_patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # Extract technical terms\n",
    "    technical_keywords = [\n",
    "        'error', 'crash', 'fail', 'down', 'slow', 'freeze', 'hang', 'timeout',\n",
    "        'connection', 'network', 'wifi', 'internet', 'server', 'database',\n",
    "        'printer', 'scanner', 'monitor', 'keyboard', 'mouse', 'laptop', 'desktop',\n",
    "        'windows', 'office', 'outlook', 'excel', 'word', 'browser', 'chrome',\n",
    "        'memory', 'disk', 'hardware', 'software', 'driver', 'update', 'install',\n",
    "        'login', 'password', 'access', 'permission', 'denied', 'expired'\n",
    "    ]\n",
    "    \n",
    "    words = text.split()\n",
    "    technical_words = [word for word in words if any(kw in word for kw in technical_keywords)]\n",
    "    \n",
    "    if len(technical_words) == 0:\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Keep only first 5 meaningful words\n",
    "    return ' '.join(technical_words[:5])\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: MOCK CHAT FUNCTION FOR DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "def mock_chat_function(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Mock chat function for demonstration. Replace with your actual chat function.\n",
    "    \"\"\"\n",
    "    # Simple pattern matching for demo purposes\n",
    "    text = prompt.lower()\n",
    "    \n",
    "    if 'blue screen' in text and 'memory' in text:\n",
    "        return 'blue screen memory error'\n",
    "    elif 'printer' in text and 'jam' in text:\n",
    "        return 'printer paper jam'\n",
    "    elif 'email' in text and 'crash' in text:\n",
    "        return 'email application crash'\n",
    "    elif 'network' in text or 'wifi' in text:\n",
    "        return 'network connection issue'\n",
    "    elif 'server' in text and 'down' in text:\n",
    "        return 'server downtime'\n",
    "    elif 'password' in text:\n",
    "        return 'authentication failure'\n",
    "    elif 'install' in text and 'fail' in text:\n",
    "        return 'software installation failure'\n",
    "    elif 'urgent' in text and 'help' in text and 'please' in text:\n",
    "        return 'meaningless'\n",
    "    else:\n",
    "        # Extract key technical words\n",
    "        technical_terms = []\n",
    "        for word in ['crash', 'error', 'fail', 'slow', 'freeze', 'timeout', 'down']:\n",
    "            if word in text:\n",
    "                technical_terms.append(word)\n",
    "        \n",
    "        if technical_terms:\n",
    "            return f\"system {technical_terms[0]}\"\n",
    "        return 'hardware malfunction'\n",
    "\n",
    "# Test the cleaning function\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING DESCRIPTION CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_descriptions = [\n",
    "    \"Hi there, my printer won't work and shows paper jam error\",\n",
    "    \"Urgent! Computer blue screen with memory error message!\",\n",
    "    \"Hello IT team, hope you're well, just wanted to report network issue\",\n",
    "    \"Please help ASAP!!! This is very urgent!!!\"\n",
    "]\n",
    "\n",
    "for desc in sample_descriptions:\n",
    "    cleaned = clean_description_with_llm(desc, mock_chat_function)\n",
    "    print(f\"Original: {desc}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: CLUSTERING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_failure_categories_with_llm(cleaned_descriptions: List[str], \n",
    "                                     chat_function, target_clusters: int = 100) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Use LLM to create failure categories from cleaned descriptions\n",
    "    \"\"\"\n",
    "    # Get unique descriptions with their frequencies\n",
    "    desc_counts = Counter(cleaned_descriptions)\n",
    "    unique_descriptions = [desc for desc in desc_counts.keys() if desc != 'Unknown']\n",
    "    \n",
    "    # Create batches for LLM processing\n",
    "    batch_size = 20\n",
    "    all_categories = {}\n",
    "    \n",
    "    for i in range(0, len(unique_descriptions), batch_size):\n",
    "        batch = unique_descriptions[i:i+batch_size]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are categorizing IT incident types. Group similar technical failures together and create category names.\n",
    "\n",
    "FAILURE DESCRIPTIONS TO CATEGORIZE:\n",
    "{chr(10).join([f\"- {desc}\" for desc in batch])}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Group similar technical failures together\n",
    "2. Create concise category names (max 5 words)\n",
    "3. Category names should be technical and descriptive\n",
    "4. Examples of good category names:\n",
    "   - \"Email Application Crash\"\n",
    "   - \"Network Connectivity Failure\" \n",
    "   - \"Hardware Driver Issue\"\n",
    "   - \"Database Connection Timeout\"\n",
    "   - \"Authentication System Error\"\n",
    "\n",
    "5. Return JSON format:\n",
    "{{\n",
    "    \"Category Name 1\": [\"description1\", \"description2\"],\n",
    "    \"Category Name 2\": [\"description3\"],\n",
    "    ...\n",
    "}}\n",
    "\n",
    "Focus on the technical root cause, not symptoms. Return only valid JSON:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = chat_function(prompt)\n",
    "            # Parse JSON response\n",
    "            batch_categories = json.loads(response.strip())\n",
    "            all_categories.update(batch_categories)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "            # Fallback: create individual categories\n",
    "            for desc in batch:\n",
    "                all_categories[desc.title()] = [desc]\n",
    "    \n",
    "    return all_categories\n",
    "\n",
    "def traditional_clustering_approach(cleaned_descriptions: List[str], \n",
    "                                  n_clusters: int = 100) -> Tuple[Dict[str, List[str]], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Alternative clustering using TF-IDF and K-means\n",
    "    \"\"\"\n",
    "    # Filter out 'Unknown' values\n",
    "    valid_descriptions = [desc for desc in cleaned_descriptions if desc != 'Unknown']\n",
    "    \n",
    "    if len(set(valid_descriptions)) < n_clusters:\n",
    "        n_clusters = len(set(valid_descriptions))\n",
    "    \n",
    "    # Vectorize the descriptions\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(valid_descriptions)\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Create categories from clusters\n",
    "    categories = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_descriptions = [desc for j, desc in enumerate(valid_descriptions) if cluster_labels[j] == i]\n",
    "        \n",
    "        if cluster_descriptions:\n",
    "            # Create category name from most common words in cluster\n",
    "            all_words = ' '.join(cluster_descriptions).split()\n",
    "            word_freq = Counter(all_words)\n",
    "            top_words = [word for word, _ in word_freq.most_common(3)]\n",
    "            category_name = ' '.join(top_words).title()\n",
    "            \n",
    "            categories[category_name] = list(set(cluster_descriptions))\n",
    "    \n",
    "    return categories, cluster_labels\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: CATEGORY ASSIGNMENT AND FINAL PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def assign_categories_to_incidents(df: pd.DataFrame, categories: Dict[str, List[str]], \n",
    "                                 chat_function) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign category names to each incident based on cleaned descriptions\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Create reverse mapping: description -> category\n",
    "    desc_to_category = {}\n",
    "    for category_name, descriptions in categories.items():\n",
    "        for desc in descriptions:\n",
    "            desc_to_category[desc] = category_name\n",
    "    \n",
    "    # Assign categories\n",
    "    how_broke_values = []\n",
    "    \n",
    "    for cleaned_desc in df_result['cleaned_description']:\n",
    "        if cleaned_desc in desc_to_category:\n",
    "            how_broke_values.append(desc_to_category[cleaned_desc])\n",
    "        elif cleaned_desc == 'Unknown':\n",
    "            how_broke_values.append('Unknown')\n",
    "        else:\n",
    "            # Use LLM to find best matching category\n",
    "            best_match = find_best_category_match(cleaned_desc, categories, chat_function)\n",
    "            how_broke_values.append(best_match)\n",
    "    \n",
    "    df_result['How broke?'] = how_broke_values\n",
    "    return df_result\n",
    "\n",
    "def find_best_category_match(description: str, categories: Dict[str, List[str]], \n",
    "                           chat_function) -> str:\n",
    "    \"\"\"\n",
    "    Find the best matching category for a description using LLM\n",
    "    \"\"\"\n",
    "    category_names = list(categories.keys())[:20]  # Limit for prompt size\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Find the best matching category for this technical description.\n",
    "\n",
    "DESCRIPTION: {description}\n",
    "\n",
    "AVAILABLE CATEGORIES:\n",
    "{chr(10).join([f\"- {cat}\" for cat in category_names])}\n",
    "\n",
    "Return only the category name that best matches, or \"Unknown\" if none fit well:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = chat_function(prompt).strip()\n",
    "        if response in category_names:\n",
    "            return response\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: MAIN PROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def process_incident_descriptions(df: pd.DataFrame, chat_function, \n",
    "                                target_clusters: int = 100, \n",
    "                                use_llm_clustering: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main pipeline to process incident descriptions and create 'How broke?' field\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING INCIDENT DESCRIPTION PROCESSING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Clean descriptions\n",
    "    print(\"\\n1. Cleaning descriptions...\")\n",
    "    df_cleaned = batch_clean_descriptions(df, chat_function, sample_size=100)\n",
    "    \n",
    "    # Step 2: Create categories/clusters\n",
    "    print(\"\\n2. Creating failure categories...\")\n",
    "    if use_llm_clustering:\n",
    "        categories = create_failure_categories_with_llm(\n",
    "            df_cleaned['cleaned_description'].tolist(), \n",
    "            chat_function, \n",
    "            target_clusters\n",
    "        )\n",
    "    else:\n",
    "        categories, _ = traditional_clustering_approach(\n",
    "            df_cleaned['cleaned_description'].tolist(), \n",
    "            target_clusters\n",
    "        )\n",
    "    \n",
    "    print(f\"Created {len(categories)} categories\")\n",
    "    \n",
    "    # Step 3: Assign categories to incidents\n",
    "    print(\"\\n3. Assigning categories to incidents...\")\n",
    "    df_final = assign_categories_to_incidents(df_cleaned, categories, chat_function)\n",
    "    \n",
    "    print(\"\\n4. Processing complete!\")\n",
    "    return df_final, categories\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: ANALYSIS AND VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_results(df: pd.DataFrame, categories: Dict[str, List[str]]):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the categorization results\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ANALYSIS OF CATEGORIZATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_incidents = len(df)\n",
    "    unique_categories = df['How broke?'].nunique()\n",
    "    unknown_count = (df['How broke?'] == 'Unknown').sum()\n",
    "    \n",
    "    print(f\"Total incidents processed: {total_incidents}\")\n",
    "    print(f\"Unique categories created: {unique_categories}\")\n",
    "    print(f\"Unknown/uncategorized: {unknown_count} ({unknown_count/total_incidents*100:.1f}%)\")\n",
    "    \n",
    "    # Top categories\n",
    "    print(f\"\\nTop 10 most common failure categories:\")\n",
    "    top_categories = df['How broke?'].value_counts().head(10)\n",
    "    for category, count in top_categories.items():\n",
    "        print(f\"  {category}: {count} incidents ({count/total_incidents*100:.1f}%)\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Category distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_15 = df['How broke?'].value_counts().head(15)\n",
    "    plt.pie(top_15.values, labels=top_15.index, autopct='%1.1f%%')\n",
    "    plt.title('Distribution of Top 15 Failure Categories')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 2: Category frequency\n",
    "    plt.subplot(2, 2, 2)\n",
    "    top_15.plot(kind='bar')\n",
    "    plt.title('Frequency of Top 15 Categories')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Number of Incidents')\n",
    "    \n",
    "    # Plot 3: Categories by service rep\n",
    "    plt.subplot(2, 2, 3)\n",
    "    rep_category = pd.crosstab(df['Service Rep'], df['How broke?'])\n",
    "    rep_category.sum(axis=1).plot(kind='bar')\n",
    "    plt.title('Incidents by Service Rep')\n",
    "    plt.ylabel('Total Incidents')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_categories\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: RUNNING THE COMPLETE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "# Replace this with your actual chat function\n",
    "def your_chat_function(prompt):\n",
    "    \"\"\"\n",
    "    Replace this with your actual LLM chat function\n",
    "    \"\"\"\n",
    "    return mock_chat_function(prompt)\n",
    "\n",
    "# Run the complete pipeline\n",
    "print(\"Starting the complete incident categorization pipeline...\")\n",
    "\n",
    "df_final, categories = process_incident_descriptions(\n",
    "    df=df, \n",
    "    chat_function=your_chat_function,  # Replace with your actual function\n",
    "    target_clusters=50,  # Reduced for demo\n",
    "    use_llm_clustering=True\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSample of final dataset:\")\n",
    "print(df_final[['Ticket', 'description', 'cleaned_description', 'How broke?']].head(10))\n",
    "\n",
    "print(f\"\\nTotal categories created: {len(categories)}\")\n",
    "print(\"\\nSample categories:\")\n",
    "for i, (cat_name, descriptions) in enumerate(list(categories.items())[:5]):\n",
    "    print(f\"{i+1}. {cat_name}\")\n",
    "    print(f\"   Includes: {descriptions[:3]}...\")\n",
    "\n",
    "# Run analysis\n",
    "top_categories = analyze_results(df_final, categories)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 9: EXPORT FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def export_results(df: pd.DataFrame, categories: Dict[str, List[str]], \n",
    "                  output_path: str = 'incident_categorization_results'):\n",
    "    \"\"\"\n",
    "    Export results to various formats\n",
    "    \"\"\"\n",
    "    # Export main dataset\n",
    "    df.to_csv(f'{output_path}_data.csv', index=False)\n",
    "    \n",
    "    # Export categories\n",
    "    with open(f'{output_path}_categories.json', 'w') as f:\n",
    "        json.dump(categories, f, indent=2)\n",
    "    \n",
    "    # Export summary\n",
    "    summary = {\n",
    "        'total_incidents': len(df),\n",
    "        'unique_categories': df['How broke?'].nunique(),\n",
    "        'unknown_count': (df['How broke?'] == 'Unknown').sum(),\n",
    "        'top_categories': df['How broke?'].value_counts().head(10).to_dict()\n",
    "    }\n",
    "    \n",
    "    with open(f'{output_path}_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Results exported to {output_path}_*.csv/json\")\n",
    "\n",
    "# Export the results\n",
    "export_results(df_final, categories)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTo use with your actual data:\")\n",
    "print(\"1. Replace 'your_chat_function' with your actual LLM endpoint\")\n",
    "print(\"2. Load your CSV file instead of using sample data\")\n",
    "print(\"3. Adjust target_clusters parameter as needed\")\n",
    "print(\"4. Run the pipeline and analyze results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
